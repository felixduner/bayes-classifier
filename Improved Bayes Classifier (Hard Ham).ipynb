{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#method for accessing files with spam/ham\n",
    "def get_files(directory):\n",
    "    return [directory + \"/\" + path for path in os.listdir(directory)]\n",
    "\n",
    "#method for reading files found\n",
    "def read_file(file):\n",
    "    with open(file, \"r\", encoding=\"Latin-1\") as f:\n",
    "        return f.read()\n",
    "\n",
    "#creating dataframe for ham mails\n",
    "df_hham = pd.DataFrame([read_file(file) for file in get_files('C:\\\\Users\\\\sarah\\\\DAT405 assignments\\\\Assignment 4\\\\HardHam')])\n",
    "\n",
    "#creating dataframe for spam mails\n",
    "df_spam = pd.DataFrame([read_file(file) for file in get_files('C:\\\\Users\\\\sarah\\\\DAT405 assignments\\\\Assignment 4\\\\Spam')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding labels, 0 for ham and 1 for spam\n",
    "df_hham['labels'] = 0\n",
    "df_spam['labels'] = 1\n",
    "df_hham.columns = ['messages','labels']\n",
    "df_spam.columns = ['messages', 'labels']\n",
    "\n",
    "#merging together to one data frame for all emails\n",
    "all_emails = df_hham.append(df_spam, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for removing headers\n",
    "for message in all_emails.messages:\n",
    "    temp = message.split(\"\\n\\n\",1)\n",
    "    if len(temp) > 1:\n",
    "        all_emails = all_emails.replace(message,temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing HTML tags and punctuation\n",
    "def pre_processor(messages):\n",
    "    messages = messages.lower()\n",
    "    no_html = re.compile('<.*?>')\n",
    "    messages = re.sub(no_html, '', messages)\n",
    "    no_long_words = re.compile('\\w{25,}')\n",
    "    messages = re.sub(no_long_words, '', messages)\n",
    "    no_special_signs = re.compile('\\W')\n",
    "    messages = re.sub(no_special_signs, ' ', messages)\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning with preprocessor before counting freq.\n",
    "count_vect = CountVectorizer(preprocessor = pre_processor)\n",
    "\n",
    "#counting freq. of different words\n",
    "bag_of_words = count_vect.fit_transform(all_emails.messages)\n",
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "#Top 20 most common words\n",
    "print(f'The 20 most common words and their freq. are: {words_freq[:20]}')\n",
    "\n",
    "#20 most uncommon words\n",
    "print(f'The 20 least common words and their freq. are: {words_freq[-20:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_emails.messages, all_emails.labels, test_size=0.35, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vectorizing data with CountVectorizer\n",
    "\n",
    "#min_df eliminates too uncommon words, max_df remove too frequent words (in % of total)\n",
    "count_vect = CountVectorizer(preprocessor = pre_processor, max_df=0.8, min_df=0.005, stop_words='english')\n",
    "\n",
    "#training set\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "print(\"X_train: \", X_train_counts.shape)\n",
    "\n",
    "#testing set\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "print(\"X_test: \", X_test_counts.shape)\n",
    "\n",
    "#counting freq. of different words in training set\n",
    "sum_words = X_train_counts.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "#Top 20 most common words\n",
    "print(f'The 20 most common words in the training set and their freq. are: {words_freq[:20]}')\n",
    "\n",
    "#20 most uncommon words\n",
    "print(f'The 20 least common words in the training set and their freq. are: {words_freq[-20:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequencies & inverse document frequency\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#training set\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(\"X_train: \", X_train_tfidf.shape)\n",
    "\n",
    "#testing set\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(\"X_test: \", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of model\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "acc_score_n = np.mean(predicted == y_test)\n",
    "print(f'The accuraccy score of Multinomial naive bayes is: {acc_score_n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "clfB = BernoulliNB(binarize=0.0)\n",
    "clfB.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of model\n",
    "predicted = clfB.predict(X_test_tfidf)\n",
    "acc_score_b = np.mean(predicted == y_test)\n",
    "print(f'The accuraccy score of Bernoulli naive bayes is: {acc_score_b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
